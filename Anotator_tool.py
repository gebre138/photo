#!/usr/bin/env python
# coding: utf-8

# In[9]:


get_ipython().system('pip install bert-tensorflow==1.0.1')
get_ipython().system('pip install bert')
get_ipython().system('pip install amseg')
get_ipython().system("pip install '/content/drive/MyDrive/bert/tool/HornMorphoA-4.3-py3-none-any.whl'")

try:
    import os
    import pandas as pd
    import glob
    import re
    import hm
    import tensorflow_hub as hub
    import numpy as np
    import gensim
    from tensorflow import keras
    from sklearn.utils import shuffle
    from sklearn.model_selection import train_test_split
    from keras.models import load_model
    from keras.layers import Activation, Dense, Dropout
    from gensim.models import Word2Vec, KeyedVectors   
    from collections import Counter
    from bert import tokenization
    import tensorflow as tf
    from amseg. amharicNormalizer import AmharicNormalizer as normalizer
except ImportError as err:
    print(err)

os.environ['TF_CPP_MIN_LOG_LEVEL'] = "2"
if not os.path.exists("dataset"):
    os.makedirs("dataset")
if not os.path.exists("dataset/stoplist"):
    os.makedirs("dataset/stoplist") 
if not os.path.exists("dataset/other"):
    os.makedirs("dataset/other") 

if not os.path.exists("dataset/stoplist/spchar.txt"):
    spch="á© á« áª á¬ á­ á® á¯ á° á± á² á³ á´ áµ á¶ á· á¸ á¹ á¶ áº á¯ á» á¼ 0 1 2 3 4 5 6 7 8 9 { } a A b B c C d D e E f F g G h H i I j J k K l L m M \
    n N o O p P q Q r R s S t T u U v V w W x X y Y z Z '"' | :\ ; , . / < > ? [ ] ; , . / á¤ á£ á¢ á¡  â€ â€œ á  á¥ á¦ á§ á¨ \ Â´ â€¦ \
    !ã€Œ "'" Â¦ _ , \ Â¨ á£ á¤ . á¹ á¢ ~ ! @ # $ % ^ & * ( ) _ + ` áŸ = - â€“ \ufeff â€¢ â˜… ğŸ™‚ ï¿½ "
    spch = spch.split()
    with open('dataset/stoplist/spchar.txt', 'a',encoding="utf-8") as file:
        for i in spch:
            file.write(i+"\n")

if not os.path.exists("dataset/stoplist/amharic_stop_lists.txt"):
    amstop="á‹¨ áˆˆ á‰ á‹šáˆ… áŠ¥áŠ•á‹° áŠáŒˆáˆ­ áŠ áŠ•á‹µ áŠ áŠ•á‹µáŠ• áŠ¥áŠ“ áŠ áˆˆ áŠ á‹¨ á‹¨á‰µ áŒáŠ á‰ áˆ‹ áˆ†áŠ áˆˆá‹¨ á‰£áˆˆ áŒŠá‹œ áˆ„á‹° á‰  áŠ áˆ áˆƒ á‹« áŒ‹ áˆ†áŠ áŠáŒˆáˆ¨ áŠá‰ áˆ¨ á‹ˆá‹­áˆ áˆ†áŠ‘ áˆ†áŠ–áˆ áŠá‹ áŠ“á‰¸á‹ áŠá‰ áˆ­ áˆáˆ‰áŠ•áˆ áˆ‹á‹­ áˆŒáˆ‹ áˆŒáˆá‰½ áˆµáˆˆ \
    á‰¢áˆ†áŠ• á‰¥á‰» áˆ˜áˆ†áŠ‘ áˆ›áˆˆá‰µ áˆ›áˆˆá‰± á‹¨áˆšáŒˆáŠ á‹¨áˆšáŒˆáŠ™ áˆ›á‹µáˆ¨áŒ áˆ›áŠ• áˆ›áŠ•áˆ áˆ²áˆ†áŠ• áˆ²áˆ áŠ¥á‹šáˆ… áŠ¥áŠ•áŒ‚ á‰ áŠ©áˆ á‰ á‹áˆµáŒ¥ á‰ áŒ£áˆ á‹­áˆ…áŠ• á‰ á‰°áˆˆá‹­ áŠ¥á‹«áŠ•á‹³áŠ•á‹µ á‰ áˆ†áŠ áŠ¨á‹šáˆ… áŠ¨áˆ‹á‹­ áŠ¨áˆ˜áˆ€áˆ áŠ¨áˆ˜áŠ«áŠ¨áˆ áŠ¨áŒ‹áˆ« áŒ‹áˆ« á‹ˆá‹˜á‰° \
    á‹ˆá‹° á‹«áˆˆ áˆ²áˆ‰ á‰ á‰°áˆ˜áˆˆáŠ¨á‰° á‰ á‰°áˆ˜áˆ³áˆ³á‹­ á‹«áˆ‰ á‹¨áŠ‹áˆ‹ á‹¨áˆ°áˆáŠ‘  áˆáˆ‰ áˆáˆ‰áˆ áŠ‹áˆ‹ áˆáŠ”á‰³ áˆ†áŠ áˆ†áŠ‘ áˆ†áŠ–áˆ áˆáˆ áˆáˆ‰áŠ•áˆ áˆ‹á‹­ áˆŒáˆ‹ áˆŒáˆá‰½ áˆá‹© áˆ˜áˆ†áŠ‘ áˆ›áˆˆá‰µ áˆ›áˆˆá‰± áˆ˜áŠ«áŠ¨áˆ á‹¨áˆšáŒˆáŠ™ á‹¨áˆšáŒˆáŠ áˆ›á‹µáˆ¨áŒ áˆ›áŠ• \
    áˆ›áŠ•áˆ áˆ°áˆáŠ‘áŠ• áˆ²áˆ†áŠ• áˆ²áˆ áˆ²áˆ‰ áˆµáˆˆ á‰¢á‰¢áˆ² á‰¢áˆ†áŠ• á‰¥áˆˆá‹‹áˆ á‰¥á‰» á‰¥á‹›á‰µ á‰¥á‹™ á‰¦á‰³ á‰ áˆ­áŠ«á‰³ á‰ áˆ°áˆáŠ‘ á‰ á‰³á‰½ á‰ áŠ‹áˆ‹ áŠ¥á‰£áŠ­áˆ… á‰ áŠ©áˆ á‰ á‹áˆµáŒ¥ á‰ áŒ£áˆ á‰¥á‰» á‰ á‰°áˆˆá‹­ á‰ á‰°áˆ˜áˆˆáŠ¨á‰° á‰ á‰°áˆ˜áˆ³áˆ³á‹­ á‹¨á‰°áˆˆá‹«á‹¨ á‹¨á‰°áˆˆá‹«á‹© \
    á‰°á‰£áˆˆ á‰°áŒˆáˆˆáŒ¸ á‰°áŒˆáˆáŒ¿áˆ á‰°áŒ¨áˆ›áˆª á‰°áŠ¨áŠ“á‹áŠ—áˆ á‰½áŒáˆ­ á‰³á‰½ á‰µáŠ“áŠ•á‰µ áŠá‰ áˆ¨á‰½ áŠá‰ áˆ© áŠá‰ áˆ¨ áŠá‹ áŠá‹­ áŠáŒˆáˆ­ áŠáŒˆáˆ®á‰½ áŠ“á‰µ áŠ“á‰¸á‹ áŠ áˆáŠ• áŠ áˆˆ áŠ áˆµá‰³á‹ˆá‰€ áŠ áˆµá‰³á‹á‰€á‹‹áˆ áŠ áˆµá‰³á‹áˆ°á‹‹áˆ áŠ¥áˆµáŠ«áˆáŠ• áŠ áˆ³áˆ°á‰  áŠ áˆ³áˆµá‰ á‹‹áˆ \
    áŠ áˆµáˆáˆ‹áŒŠ áŠ áˆµáŒˆáŠá‹˜á‰¡ áŠ áˆµáŒˆáŠ•á‹á‰ á‹‹áˆ áŠ á‰¥áˆ«áˆ­á‰°á‹‹áˆ áŠ¥á‰£áŠ­á‹ áŠ áŠ•á‹µ áŠ áŠ•áŒ»áˆ­ áŠ¥áˆµáŠªá‹°áˆ­áˆµ áŠ¥áŠ•áŠ³ áŠ¥áˆµáŠ¨ áŠ¥á‹šáˆ áŠ¥áŠ“ áŠ¥áŠ•á‹° áŠ¥áŠ•á‹°áŒˆáˆˆáŒ¹á‰µ áŠ¥áŠ•á‹°á‰°áŒˆáˆˆáŒ¸á‹ áŠ¥áŠ•á‹°á‰°áŠ“áŒˆáˆ©á‰µ áŠ¥áŠ•á‹°áŠ áˆµáˆ¨á‹±á‰µ áŠ¥áŠ•á‹°áŒˆáŠ“ á‹ˆá‰…á‰µ áŠ¥áŠ•á‹²áˆáˆ \
    áŠ¥áŠ•áŒ‚ áŠ¥á‹šáˆ… áŠ¥á‹šá‹« áŠ¥á‹«áŠ•á‹³áŠ•á‹± áŠ¥á‹«áŠ•á‹³áŠ•á‹³á‰½á‹ áŠ¥á‹«áŠ•á‹³áŠ•á‹· áŠ¨ áŠ¨áŠ‹áˆ‹ áŠ¨áˆ‹á‹­ áŠ¨áˆ˜áŠ«áŠ¨áˆ áŠ¨áˆ°áˆáŠ‘ áŠ¨á‰³á‰½ áŠ¨á‹áˆµáŒ¥ áŠ¨áŒ‹áˆ« áŠ¨áŠá‰µ á‹ˆá‹˜á‰° á‹ˆá‹­áˆ á‹ˆá‹° á‹ˆá‹°áŠá‰µ á‹áˆµáŒ¥ áŠ¥á‰£áŠ­áˆ¸ á‹áŒª á‹«áˆˆ á‹«áˆ‰ á‹­áŒˆá‰£áˆ á‹¨áŠ‹áˆ‹ á‹¨áˆ°áˆáŠ‘ \
    á‹¨á‰³á‰½ á‹¨á‹áˆµáŒ¥ á‹¨áŒ‹áˆ« á‹« á‹­á‰³á‹ˆáˆ³áˆ á‹­áˆ… á‹°áŒáˆ á‹µáˆ¨áˆµ áŒ‹áˆ« áŒáŠ• áŒˆáˆáŒ¿áˆ áŒˆáˆáŒ¸á‹‹áˆ áŒá‹œ áŒ¥á‰‚á‰µ áŠá‰µ á‹°áŒáˆ á‹›áˆ¬ áŒ‹áˆ­ á‰°áŠ“áŒáˆ¨á‹‹áˆ á‹¨áŒˆáˆˆáŒ¹á‰µ á‹­áŒˆáˆáŒ»áˆ áˆ²áˆ‰ á‰¥áˆˆá‹‹áˆ áˆµáˆˆáˆ†áŠ áŠ á‰¶ áˆ†áŠ–áˆ áˆ˜áŒáˆˆáŒ¹áŠ• áŠ áˆ˜áˆáŠ­á‰°á‹‹áˆ \
    á‹­áŠ“áŒˆáˆ«áˆ‰ áŠ á‰ áˆ«áˆ­á‰°á‹ áŠ áˆµáˆ¨á‹µá‰°á‹‹áˆ áŠ¥áˆµáŠ¨ á‹­áˆ… áŠ¨áŠ á‹«áˆˆ á‹ˆá‹° áˆµáˆˆ á‰°áˆ« áˆ™áˆ‰ áŒ‹áˆ­ áŠ¥áŠ“ áŠá‹ áŒáŠ• á‹ˆá‹­áˆ áŠ¥áŠ•áŒ… áŠ¥áŠ•áŠ³ áŠ“á‰¸á‹ áŠ á‹áŠ• áŠ¥áŠ•á‹²áˆ… áŠ¥áŠá‹šáˆ… áˆáŠ• á‹­áŠ¸á‹áˆ"
    amstop = amstop.split()
    with open('dataset/stoplist/amharic_stop_lists.txt', 'a',encoding="utf-8") as file:
        for i in amstop:
            file.write(i+"\n")

if not os.path.exists("dataset/other/complex_word.xlsx"):
    root="áŠ®áˆµáˆ›áŠ“ áˆ˜á‰ƒáˆ­ á‹ˆá‹°á‰¥ á‰†áˆ¨áˆá‹° áˆˆáˆáŒ½ á‹°á‰¦ áˆˆáˆ°áˆ° áˆˆáˆ°áŠ áˆˆáˆ´ áˆˆá‰†á‰³ áˆˆá‰ á‰… áˆˆá‰ áŠ• áˆˆá‰ á‹° áˆˆá‰°á‰° áˆˆáŠ¨á‰µ áˆˆáŠ°áˆ áˆˆá‹˜áˆˆá‹˜ áˆˆá‹˜á‹˜ áˆˆáŒˆáˆ˜ áˆˆáˆáˆ áˆá‰£á‰¥ áˆ˜áŠá‹°áŒˆ áˆáŒˆá‹µ áˆáŒá‹šá‰µ áˆ˜áˆáŒ¨áŒ­ áˆ¨áˆ˜áŒ¥ áˆ¨á‰€á‰€ \
    áˆ¨á‰¥áŒ£ áˆ˜á‰ƒá‰ƒáˆ­ áŠ¥áŠ•áŒ­áŒ­ áŒˆáŠáŠ áˆ­á‹°á‰µ áˆ­áˆµá‰µ áˆ­á‰±áŠ¥ áŠ áˆ­áŠ¥áˆµá‰µ áˆ®á‰„ áˆ°áˆˆá‰€ áˆ°áˆˆá‰  áˆ°áˆˆáŒ  áˆ°áˆ‹áŒ¤ áˆ°áˆ¨áŠá‰€ áˆ˜áˆµáˆ¨áŒ áˆ˜áˆµáˆ¨áŒ½ áˆ°á‰€áŒ  áˆ°á‰ á‰€ á‰°áˆ°áŠ“áˆ°áˆˆ áˆ°áŠá‰ áŒ  áˆ°áŠ•á‰ áˆ­ áˆ´áˆ« áˆµá‹µ áˆ°áŒ‹áˆ­ áˆ°áŒˆáŠá‰µ áˆ°áŒá‹³á‹³ áˆ¨á‰£á‹³ áˆ²áˆ«áŠ­ \
    áˆ²áˆ³á‹­ áˆ³áŠ•á‰ƒ á‰£á‹­á‰°á‹‹áˆ­ áˆ´áˆ°áŠ› áˆˆá‰€áˆˆá‰€ áˆ‹áˆ¸á‰€ á‹ˆáˆ¨á‰µ á‰…á‹­áŒ¥ áŒ‹áˆ¬áŒ£ áˆ˜áˆ›áˆµ áŒˆáˆ¨áˆ¨ áŠ¡á‹°á‰µ áá‹­á‹ áŠ áˆ¸áˆˆá‰  áˆ¸áˆˆá‰° áˆ¸áˆ˜á‰€ áˆ¸á‰€áŒ  áˆ¸áˆáŒ  á‰€áˆˆáˆ° á‰€áˆ¸áˆ¨ áŠ¨áˆ¸áŠ á‰€á‰°áˆ¨ á‰€áˆ°áˆ¨ á‰€áŠáŒ áˆ° á‰€áŠáŒ¨áˆ¨ á‰€áŠ–áŠ“ áˆˆá‰ áŒ  á‹°áŒáˆ° áŒáˆ« \
    áˆ˜áˆµáŠ­ áˆ˜áˆ¨áŠ• áˆ°á‹¨áˆ˜ á‰°áˆ¾áˆ˜ á‰‹áˆ³ á‰£á‰°áˆŒ á‰£á‹áˆ« á‰°áˆ°á‰ áŒ£áŒ áˆ¨ áŠáˆáŒˆ á‰°á‹°áˆ«áˆ² áˆáŠ“á‰¥ áŠ á‹°áˆ˜ áŠ•á‹‹á‹­ áŠ•á‰áŒ áŠ áˆˆáˆ˜ áŠ áˆˆá‰£ áˆ›áˆ˜áŠ•á‹ áŠ­ áŠ áˆšáŠ«áˆ‹ áŠ áˆá‰£áˆ¨á‰€ áˆ˜áŠá‹˜áˆ¨ áˆ˜áŒ¥áŠ” áˆ€áˆ©áˆ­ áŠ áˆá‰£ á‰…áˆ­áŠá‰µ á‹°áˆˆáˆ áŠ­áˆá‰½á‰µ áˆ¨á‰‚á‰… \
    áŒ¥á‰…áŒ¥á‰… áŠ¥áŠ•á‰ áŠ á‰€á‰  áˆˆá‹°áˆ á‰‹á‰µ áŠ áŠ¨áŠ•á‹áˆ½ áŒá‹µá‹áˆª áˆ»á‰€áˆˆ áˆ¸áŠ¨áŠ á‹ˆáŒ£áˆ« áˆ°á‹‹áˆ« áŠ áˆáŠ«áˆ¸ á‰°áˆ˜áˆ˜ áŒáŒ áŠ á‹°á‰¥ áŠ©á‰³áŒˆáŒ áˆ á‹ˆáˆ¨á‹› á‹ˆá‰ á‰… á‹°áŒ“áˆ³ áŒ…á‰¥áˆ« áŒ¥áŒˆá‰µ á‹á‹­á‹³ áˆ€áŒˆáˆ¨áˆ°á‰¥ áˆáˆˆáŠ•á‰°áŠ“ áˆáŠáŠ› áˆƒáˆŒá‰³ áˆ€áˆ˜áˆáˆ›áˆ \
    á‹­áˆµáˆ™áˆ‹ áˆ‰áŠ áˆ‹á‹Š áˆáŒ‹ áˆ‰áŒ¤ áˆáˆ«áŒ… áˆ€áˆ´á‰µ áˆ˜áˆ³ áˆ›áŠ¥áˆ¨áŒ áˆáŠ•á‹áˆ­ á‰ƒáŠ•á‹› á‰ áˆˆá‰°á‰° á‰¡ááŠ“ á‰¦áˆ¨á‰€ á‰§áˆá‰µ áˆ˜áŒ‹á‹¨á‰µ á‰£áˆ‹áŠ•áŒ£ á‰†áˆ¨á‰†áˆ° áŠ¥áˆá‰£á‰µ á‰°áŒáˆ³áŒ½ á‰°áŒ áŠ“á‹ˆá‰° á‰¸áˆá‰°áŠáŠá‰µ á‰¸áŠáˆáˆ­ áŠáˆ¸áŒ  áŠá‹áŒ¥ áŠá€á‰¥áˆ«á‰… áˆ¨á‰ áˆ¨á‰  \
    áŠ áˆ¨áˆá‰€ áŠ áˆ­áˆáˆ áŠ á‰°áˆ˜ áŠ á‹˜á‰¦á‰µ áŠ áŒˆáŠ“ áŠ áŒá‰¦áŠ› áŠ áŒ­áˆáŒ áŠ¢áˆáŠ•á‰µ áŠ¥á‰…áŒ©áŠ• áŠ áŠ¨á‰°áˆ˜ áŠ¥á‹áŠ• áŠ¥áŠ•á‰¦á‰€á‰…áˆ‹ áŠ¨á‹ˆáŠ á‹ˆáˆ¨áŒá‰¡ á‹šá‰€áŠ› á‹›á‰ áˆ¨ á‹áˆ› á‹­á‹ á‹°áˆ¨áˆ˜áŠ• á‹µá‹µáˆ­ áŒ€áˆŒ áŒˆáˆ­áŒƒá‹ áŒáŠá‰†áˆˆ á‰°áŒáˆ³á‰†áˆˆ áŒ¥áˆáŠ“ áŒ¨áˆ¨áá‰³ \
    áŒ­á‰¥áŒ¥ áŒ¸á‹³áˆ áŒ½áˆáˆ˜á‰µ á‹ˆáŠ“ á‹ˆáˆ› áŒ½áŒŒ áˆá‰°á‰° áˆáŒˆáˆ˜ áˆáŒˆáŒ  ááˆ­áŠ©á‰³ áŒáˆ¬ ááŒ áŠ“ áˆ€áŒ« áˆ€áŠ¬á‰µ áˆá‹³á‹µ áˆ…áˆ‹á‹Œ áˆ…á‰¡á‹• áˆˆá‹áˆ‹á‹‹ áˆˆáˆ†áˆ³áˆµ áˆˆá‰†áŒ  áˆŒáŒ£ áˆáŒ¨áŠ› áˆá‰€á‰µ áˆáŒ¥ áˆ˜áˆ°áˆª áˆ˜á‰ƒáŠ áˆ˜áˆ¨áŠ€ áˆ˜á‰…áŠ– áˆ˜áŠá‹˜áˆˆ \
    áˆ™á‹³ áˆ˜áˆ­áŒ áˆ˜á‹µá‰¥áˆ áˆ˜á‰ áˆˆá‰µ áˆ™áŠ“ áˆáŒ¸á‰µ áˆ›áˆ­áŒ£ áˆ›áá‹³ áˆ˜á‹¨áˆ° áˆŸáŒ¨ áˆ¨á‰ á‰  áˆ¨á‰¥ áˆ˜áŒ«á‰µ áˆ«á‹° áˆµáŠ“á‹³áˆª áŒá‹á‰µ á‰°áˆ°áŠá‰€ áˆ°áŠá‰€ á‹µá‰¥áŠá‰µ á‰€áˆ¨áˆ¨ áŠ¨áˆ­áˆµ áŠ áˆá‹µ áˆ€áˆ³á‹Š á‰€áŒ‹ áˆ¹áˆ áˆƒáˆ³á‹Š áˆ½á‰¥áˆá‰… áˆ¸áŠá‰†áŒ  \
    áˆ¾áˆ˜áŒ áˆ¨ áˆ»áŒ‰áˆ« áˆ¾á‰°áˆ áˆ¸ááŒ¥ á‰€áˆá‰¥ á‰…áˆáŒ¥áˆ á‰€áŠ•áŒƒ á‰€á‰ áŠ› á‰€á‹¨á‹° á‰áŠ•á‹³áˆ‹ á‰áŠ•áŒ½áˆ á‹áˆ­áŒ­ á‹áˆ€ á‰°áŠ“áŒ‹ áŒ áŠ” áŒ‰á‹« á‰°áŒá‹³áˆ®á‰µ á‰áŠ•áŒ£áŠ• á‹ˆáˆá‰… áŠ“áŒ  áˆµáˆá‰³ áŠ¨áŒ€áˆˆ áˆ˜áŒˆáˆ­áˆ˜áˆ áŒ‰áˆ«áˆ›á‹­áˆŒ á‹°áˆá‰ƒá‰ƒ áŒ½áŠ•áˆáŠ› áˆ˜á‰°á‹¨á‰¥ \
    áˆ…áŒ¸áŒ½ áˆŸáˆ­á‰µ áˆŒáˆ›á‰µ á‹™á‹áŠ• áˆáˆµá‰…áˆáŠ“ á‹ˆáˆˆáˆ ááˆ­áˆáˆ« áˆ˜áŠ– áŠ áˆ½áŠ¨áˆ­ á‰½áˆ®á‰³ áˆáŒ¨áˆˆáˆ á‰€áˆˆá‰ áˆ° áˆ¸áˆˆá‰€á‰€ áˆ¸áˆ˜áŒ áŒ  á‹˜áŠáŒ áˆˆ á‰¸áŠáŠ¨áˆ¨ áˆáŠá‹°á‰€ á‹˜áˆ¨áŒ áŒ  á‹ˆáˆ°áˆˆá‰° áŠ•á‰µáˆ­áŠ­ áˆ°áˆˆá‰€áŒ  áˆ­áˆšáŒ¦ áŒ áŒ áˆ¨ á‰°áŒáŠáŒ¨ á‰°á‹°áˆ˜áˆ˜ \
    áŠ‘á‰£áˆ¬ áŠ áˆœáŠ¬áˆ‹ áŠ áˆ½áŠ­áˆ‹ áŠ áˆ½áŠ«áŠ« áˆ˜áˆ³áˆˆáŒ¥ áŠ¥á‰¥áˆªá‰µ áˆˆá‹‹áˆ³ áŠá‹› áˆáŠ¨á‰µ áˆˆáˆá‰¦áŒ­ áˆŠá‰… áˆŒáŒ¦ áˆáˆ»áŠ• áŠ¥áŒ­ áˆá‹á‰ áˆ« áˆ˜á‹˜á‹ áˆ˜á‹²áŠ“ áˆ˜áŒáˆ‹áˆŠá‰µ áˆáŒ¥á‹‹á‰µ áˆ™áˆ¬ áˆ™áˆ¾ áˆ›áˆ­á‹³ áˆ›á‰… áˆ›áŠ¥á‰€á‰¥ áˆ›áŠ¥á‹˜áŠ• áˆ›áŒ áˆ›áŒ¥ áˆáˆ…á‹‹áˆ­ \
    áˆáŠ­áˆ¸ á‹˜áˆ€ áˆµáˆ­á‹ˆ áˆµá‹áˆ­ áˆ­á‹áˆ«á‹¥ áˆ¨áŒáˆ¨áŒ áˆªá‹ áŠ¥áˆ­á‰ƒáŠ• áˆ¬á‰µ áˆ®áˆ® áˆ°áŠá‹µ áˆ°áˆáŠ áˆ²á‰£áŒ áˆ³á‹±áˆ‹ áˆµáˆá‰µ á‹áˆ á‰£áˆáŠ•áŒ€áˆ« áˆ˜áˆ½áˆˆá‰µ áˆ½áˆ­á‹°á‹³ áˆ¸áˆ¸ áˆ¸á‰€áˆˆ áˆ½áŠ•áŒˆáˆ‹ áˆ¸áŠáˆ¸áŠ áŒ…áˆ¨á‰µ áˆ¸áŠ•áŒ áˆ»áŒˆá‰° áˆ…á‰¥áˆ­ á‰€áˆˆá‰¥ áˆ˜á‰€áˆá‹ˆáŒ¥ áŠ®áˆ¨á‹³ \
    á‰…áˆµáˆá‰µ á‰…áˆ«áˆª áŠ•áŒ¥áˆ­ á‰ƒáˆ¨áˆ˜ á‰…áŠ•áŒ£á‰µ á‹áŒ¥áŠ•á‰…áŒ¥ á‰‹áŒ¥áŠ á‰¢áŒ¤ á‰¥áˆ‹áˆ½ á‰¥áˆ‹á‰´áŠ“ á‰¡áŒ«á‰‚ á‰µáˆáˆ á‰°áˆ˜áŠ• á‰°áˆ¨á‰¥ á‰€á‹áˆµ áŠ•áˆ¨á‰µ áˆ˜á‰°á‰¸á‰µ áŠ«áŠ á‹ˆáŠ¨á‰£ áˆ˜á‹˜áŠ¨áˆ­ áŠ¥áˆ­á‰… á‰µá‰¢á‹« á‰µáˆ­áˆáˆµ á‰¥áŒ£áˆª áŒˆá‰ áˆ¨ áŒ‰á‰¦ áŒá‰µáˆ­ áŒˆá‹µ \
    á‹˜áŒˆáˆá‰°áŠ› áŒ­ááˆ« áˆ½áŒáŒáˆ­ áŒ‰áˆáˆ‹á‰µ áŒáˆ½á‰ á‰µ áˆ˜áŠ¨á‰³ áŒá‰¥á‹ áŒáŠ¡á‹ áŒáˆ°á‰ˆáˆˆ áŒ¥áˆ­áŠ á‰£áˆˆáˆŸáˆ á‰£áˆá‹°áˆ¨á‰£ á‰µáŠ«á‹œ á‰µáŠ¥á‹­áŠ•á‰µ áŒá‰¥áŠ á‰µ áˆ˜áŠá‰£áŠ•á‰¥ áŠ áˆ‰á‰£áˆá‰³ áŠ áˆ‰á‰³ áŠ á‹ˆáŠ•á‰³ áŠ áˆ³áˆ­ áŠ áˆ­áŒ©áˆœ áŠ áˆ»áˆ« áŠ áˆ½áˆ™áˆ­ áŠ á‰ áˆ \
    á‹³á‰£ áŠ á‰¦áˆ áŠ¥áŠ­áˆ áˆ›áŠá‰† áŠ¥áŠ•áŒá‰» áŠ•áŠ¡áˆµ áŠ áŠ­ááˆá‰°áŠ› áˆáŠ«á‰³ áŠ á‹°áˆˆ áŠ ááˆ‹ áŠ¥áˆááŠ á‹ˆáŒ áŠ á‹áˆ‹áˆ‹ áŠ¥áˆá‰¡áŒ¥ áŠ¥áˆá‰… áŠ¥áˆµáˆµá‰µ áŠ¨áˆ°áˆ˜ áŠ¬áˆ‹ áŠ®áˆ¨á‰¥á‰³ á‹ˆáˆ‹áˆáŠ• á‹á‰…áˆ«á‰µ áˆ˜á‹‹á‰…áˆ­ á‹ˆáŠ•á‰ á‹´ á‹ˆáŠ•á‹°áˆ‹áŒ¤ á‹ˆá‹˜áŠ“ á‹ˆáŒ€á‰¥ \
    á‹ˆáŒŒáˆ» á‹˜á‹¨áˆ¨ áˆ˜á‹áˆá‰… á‹µáˆá‹ á‹°áˆá‰¥ á‹µáˆ­á‹µáˆ­ á‹°á‰£ á‹°á‰£áˆ á‹°á‰¥áˆ­ á‹°á‰¥á‹› á‹°áŠ•á‰³ á‹°á‹Œ á‹°áŒ€áŠ• á‹µáŒáˆ› á‹°áˆáˆ¨áˆ° á‹±á‰¤ á‰€áˆ˜áˆ˜ á‹³á‹‹ á‹µáŠ•áŠ­ áŒˆá‰£áˆ­ áŒáˆµá‰‹áˆ‹ áŒ áŠ•á‰… á‰°á‹°áˆˆá‰€ áˆ˜áˆ‹á‹ˆáˆµ á‰£áˆˆáŒ áŒ‹ áŒ­áˆ‹áŠ•áŒ­áˆ áŒ¸áŠ‘ áŒ½á‹‹ ááˆáˆ°á‰µ \
    áˆáŠáŒ á‹˜ áˆá‹áˆµ ááŠ«á‰µ á‹áˆµáŠ® á‹áŠ–áˆµ áŒá‹ ááˆµáˆ€ á‰ áˆ¨á‰ áˆ¨ á‰µá‰½á‰µ áŠ á‰ áˆ³ áŠ áˆˆáŠá‰³ áŠáŒá‹´ áŠ á‹ˆáŠ¨ áˆ¸áˆ­áŠ­ á‰µáŠ•á‰ á‹« áŠ áŠ•áŒ“ á‰€áˆ¨á‰€áˆ¨ á‹‹áˆá‰³ áˆ…á‹³áŒ á‹«áˆ¸á‰ áˆ¨á‰€ áˆ°áˆ­áŒ¥ áŒ‰á‰¶ áˆ˜áŠáˆ˜áŠ á‰€á‹® á‹ˆá‰µáˆ® á‰£á‹á‹› á‹˜áˆ¨áˆ¨ á‹²á‰ƒáˆ‹ á‰¦áŒ­á‰§áŒ« \
    á‰†áŒ­á‰‹áŒ« áˆ½áˆ™áŒ¥ áˆ°áˆ°áŠ á‰°áˆáŠ“áˆáŠ áŒ‹áŒ  áŠ¨áŠáŠ¨áŠ áŠ áŠ•áŠ³áˆ­ á‰°áˆ˜áˆ³á‰€áˆˆ áŠ á‹ˆáŒˆá‹˜ áŒˆáˆ­ áŒˆá‰³ áˆ°áŠáŠ¨áˆˆ á‰°áŠ“áŒ  áŒáˆ…á‹°á‰µ áˆ›áˆ¾áˆ­ áˆ˜á‹áŒˆáŠ• á‹ˆáˆ¨á‰³ áŒˆáˆ€á‹µ áŠ áˆ­á‰³áŠ¢ áŒˆáˆá‰µ áŠ áŒ€á‰¥ áˆ…á‰¡áŠ¥ á‰‹áŒ¨ á‰µáŒ‰ áˆ¨áŒˆá‹µ á‹³á‹´ áˆ˜áŠ¨á‰° á‹ˆáˆ¨áˆ° \
    áˆ­áˆ…áˆ©áˆ… áŒ¥áˆá‰… á‰°áŒˆá‰£á‹°á‹° á‹ˆá‰€áˆ³ á‰€áŠ•á‰ áˆ­ áˆˆáŒˆáˆ° áˆáˆ½áŒ áŠ áŠ•áŒ‹á‹³ áˆ˜áŠ“á‹ˆá‹ áˆ˜áˆáŠáˆ½áŠáˆ½ á‰°áŠ•áŒ£áŒ£ á‹á‹ á‹˜áŠ¨áˆ¨ áŠ áˆ› á‰³á‰€á‰  áŠ áŒˆá‰° áˆ˜áŠá‰€áˆ¨ áˆ°áˆ¨áŒ¸ áŠ áˆ¾á‰€ áŒ¥áˆ» áˆ›áŠ› áˆ…áˆá‹áŠ“ áˆá‰… áˆˆá‹˜á‰  á‰°áˆ˜áˆ¨á‹˜ áŠ áˆáˆ³áˆ áˆ˜áˆ²áŠ“ áˆ˜á‰ƒ \
    áˆ˜áŠ“áŠ áˆ˜áŠ«áŠ• áˆ›áŒˆáŒ  áˆµáˆá‰» áˆ°áˆ˜áˆ¨ áˆµáˆµ áˆ±á‰£áŠ¤ áˆµáŠ•á‹± áˆµáŒ‹á‰µ áŠ áˆ°áŒ‹ áˆ¾áˆˆ áˆ½áŠ•áˆ½áŠ• á‰‚áˆ áˆ»áŠ•á‹³ á‰€áˆ‹á‹µ á‰†áˆ¨á‰†áˆ¨ á‰†áˆ¨á‰†á‹˜ á‰…áŠá‰µ á‰¥áˆ„áˆ«á‹Š á‰£áˆá‰¦áˆ‹ á‰£áˆá‰´á‰µ á‰¥áˆ©áˆ… á‰£áˆ­áŠ”áŒ£ áŠ á‰ áˆ°áˆ¨ á‰¦á‰ƒ á‰ áŠáŠ á‰£áŠáŠ á‰ á‹ˆá‹˜ á‰ á‹³ á‰§áŒ áŒ  \
    á‰µáˆá‰µ á‰³á‰ á‹¨ á‰³á‰³áˆª á‰µáŠ¥á‰¢á‰µ á‰³á‹› á‰µá‹á‰¥á‰µ á‰°á‹µáˆ‹ á‰°áŒˆáŠ• áˆ˜áŠ•á‰ áˆ­ áŠ áˆˆá‰ áŠá‰  áŠ áŠ“á‹ˆáŒ  áŠáŒ á‰  áŠáŒ áˆ áŠ á‹á‹µ áˆáŠ¥áˆ‹á‹µ áˆáŠ¥áˆ˜áŠ• áŠ¥áˆ™áŠ• áŠ¥áˆ˜áŒ«á‰µ áŠ áˆ¨áˆ˜áŠ” á‹ˆáŒˆáŒá‰³ á‹‹á‰¢ á‹˜á‰ á‰µ áˆˆáŒáŒ¥ á‰¦á‰°áˆ¨áˆ á‹˜á‰€áŒ  á‰°á‹›áˆ˜á‰° \
    á‹˜áˆˆáˆ° á‹˜áˆˆáˆ á‹˜áˆ‹á‰ á‹° áŠ á‹‹á‹› á‹áˆ½áŠ•ááˆ­ áŠ á‹á‰³áˆ­ áŠ áˆ¸á‰ áˆ¨á‰€ á‹˜áˆ¨áŒ¦ á‰€áˆ‹áˆ˜á‹° áŠ¨áŠá‰ áˆˆ á‹ˆáˆ¨áŠ•áŒ¦ á‹ˆáˆ˜áŠ” áŠ¨á‹­áˆ² á‹ˆáˆ®á‰ áˆ‹ á‹ˆáˆ¨áŒ‹ áŠ¨áŠá‰¸áˆ¨ áŠ¨áŠáˆ áŠ¨á‰°á‰  á‹ˆáˆ°áŠ« á‹ˆá‹­á‰£ á‹ˆáŒ áŠ á‹ˆáŒˆáŠ á‹ˆáŒˆáˆ¨ áŠ¨áˆ˜áŠ¨áˆ˜ áŠ¨áˆ¨á‹°á‹° áŠ¨áˆá‹‹áˆ³ áŠ¨áˆ‹á‰£ \
    á‹ˆá‹˜áˆ á‰°á‹ˆáŠáŒ¨áˆ á‹ˆáŠ” á‹ˆá‹°áˆ˜ áŠ á‰†áˆ«áŠ˜ á‹‹áˆˆáˆˆ áˆ¸á‰€áˆ¸á‰€ áŠ¥áá‹­á‰³ áŠ¨áˆˆá‰  á‰¸áˆ­ á‹á‹á áˆˆá‹˜á‰¥á‰°áŠ› á‰€áˆ³áˆ› á‰µáˆáŠ­áˆ…á‰µ áŒ¥á‰ƒá‰…áŠ• áˆ€á‰°á‰³ áˆ‚áˆµ áŒ á‰ á‰¥á‰µ áˆáˆ³áŠ• áˆáˆŒ áˆ˜áˆ°á‹‹á‰µ áˆ˜áˆµá‹‹á‹•á‰µ áˆ˜á‰£ áˆ˜áŠ•áŒ‹ áˆ˜á‹°á‹³ áˆ˜á‹µáŠ• \
    áˆ›áˆ³ áˆ›áŠ¥á‰ áˆ áˆ°áˆˆá‰£ áˆ½áˆ˜á‰ƒ á‰¤á‹› áŠá‰ áˆá‰£áˆ áŠ áˆ‹á‰£ áŠ á‰€á‰ á‰µ áŠ á‰» áŠ áŠ•áŒ‹á‹ áŠ á‹µáˆ› áŠ áŒ‹á‹áˆª áŠ áˆáŠ•áŒ‹áŒ­ áŠ­áˆ­á‰³áˆµ áŠ­áˆá á‹‹áˆµá‰µáŠ“ á‹‹áŒ á‹áŒ¥áŠ• á‹µáŠ•áŒ‹áŒŒ áŒ¥áˆªá‰µ áˆáˆ­ ááŠ•áŒ­ áˆ°áŒ‹ á‰ƒáŠ˜ á‰°áˆ³áŠ á‹˜áŠáŒ‹ á‹›áˆˆ á‹°áŠáŒˆáŒˆ á‰°áŠ•áŒ¸á‰£áˆ¨á‰€ \
    á‰°áŠ¨áˆ›á‰¸ á‰°áŠ¨áˆ°á‰° á‰°á‹‹á‰€áˆ¨ áŒ€áŠ•áˆáˆ á‹ˆáŒ‹á‹³ áˆ°áŒˆá‰£ áˆ°á‰ áŠ¨ áˆ°áŒˆáˆ°áŒˆ á‰€áˆˆáˆ˜ áŠ“áˆ™áŠ“ á‰µáˆ­áŠ¢á‰µ áŠ¥áŒ© áˆ¸áˆ¨áŠ› áŠ¥áŠ•á‹áˆ‹áˆ áŠ¨áˆ¨áŠ¨áˆ° á‹á‹µáˆ› á‰°á‹˜áŠ¨áˆ¨ á‹˜áŠ¨á‹˜áŠ¨ á‹˜á‹á‹µ á‹˜á‹­á‰¤ á‹›á‰» á‹°áˆˆáˆ° á‹°á‰ƒ á‹°áˆˆáˆ˜ á‹°áˆˆá‰  á‹°áˆˆá‹˜ á‹µáˆªá‰¶ á‹µáˆª \
    á‹°áˆ°á‰€ á‹°á‰ á‰° á‹°á‰€áŠ á‹°áŒˆáŠ á‹°á‰€á‹°á‰€ á‹°á‰ áˆˆ á‹°á‰£á‹­ á‹°áŠá‰ á‹˜ á‹°áŠá‹ á‹°áŒˆáˆˆáˆˆ á‹µáŠ•áŒ‰áˆµ á‹°áˆá‰€ á‹°áˆáŒ áŒ  á‹³áˆ° á‹°áˆáˆ­ á‹³áˆ¸á‰€ á‹³á‰ áˆ¨ á‹³á‰ áˆ° á‹³á‰°áŠ› á‹³áŠ¨áˆ¨ áˆ‹á‰†áŒ  áŒáˆµá‰³ á‹µáŠ•á‹á‰³ á‹¶áˆˆá‰° áŠ á‰ƒáŒ£áˆª áŒ…áˆáˆ‹ áŒ€áˆá‰ áˆ­ áŒ€á‰¥á‹µ áŒƒáŒ€ \
    áŒˆáˆˆáˆ˜áŒ  áŒˆáˆˆáˆáŒ  á‰°áŒˆáˆ›áˆ¸áˆ¨ áŒˆáˆ¨á‹˜á‹˜ áŒˆáˆ¨áŒˆáˆ¨ áŠ áˆ˜áŒ¸ áŒˆáˆ°áŒˆáˆ° áŒˆáˆ°áŒ¸ áŒˆáˆ¸áˆˆáŒ  áŒˆáŠá‰°áˆ¨ áŒáˆ‹ áŒˆáŒ áŒ  áŒá áŒá‹™á áŒáˆ¨áŒ  áŒáˆ¨áŒáˆ¨ áŒáˆ°á‰†áˆˆ áŒáŠáŒ  á‰°áŒáŠ“áŒ¸áˆ áŒáˆáŠáŠ áŒáˆá‹¨ áŠ áŒ“áˆ« áŠ áŒáˆ¨ áŒ áˆ€áŠ˜ áŒ áˆˆáˆˆ áŒ áˆˆáˆ˜ áŒ áˆáˆ°áˆ á‰°áŒ¥áˆˆá‰€áˆˆá‰€ \
    áŒ áˆˆá‹˜ áŠáˆ¨á‰° áŒ áˆ¨áˆ¨ áŒ áˆ¨áˆ˜áˆ° áŒ áˆ¨áŠá‰€ áŒ áˆ¨áˆ á‰€áˆ¨á‰€á‰  áŒ áˆ°áŒ áˆ° áŒ á‰¢á‰¥ áŒ áŠá‰ áˆ° áŒ áŠáŠ áŒ áŠá‹› áŒ áŠáŒ áŠ áŒ áŠáˆáˆ áŒ áŠ“ áŒ½áŠ“á‰µ áŠ áŒ½áŠ“áŠ“ áŠ áŒ¥áŠ“á áŒ á‹ˆáˆˆáŒˆ áŒ áŒˆáŠáŠ á‹˜áŒˆáŠáŠ áŒ£áˆ˜áŠ áŒ£áˆ¨ áŒ¦áˆ›áˆ­ áŒ¦áˆ á‹°áˆ« áŒ¨áˆ˜á‰° áŒ¨áˆ˜á‰°áˆ¨ \
    áŒ¨áˆ˜á‹°á‹° áŒ¨á‰¦á‹° áŒ¨áŠáŒˆá‹˜ áŒ¨á‹°á‹° áŒ¨áˆáŒˆáŒˆ áŒ«á‰°áˆ¨ áŒ­á‰¦ áŒ¸áŠ“ áˆáˆˆáˆ° áˆáˆˆá‰€áŒ  áˆáˆˆáŒ áˆáˆáˆáˆ‹ áˆáˆ¨áŒ… áˆáˆ¨áŒ áŒ  áˆá‰°áŒˆ á”á‹³áŒáŒ‚ á‰°áˆ°áŒˆáˆ°áŒ áŒ®á‰¤ áŒ¨áŠ¨áŠ¨ á‰°áŒ¨áŠ“áŒáˆˆ áŠ®áˆ­áˆ›á‰³ áŒ¨áˆ¨áˆ áŒ¨áˆ¨áŒˆá‹° áŒ§áˆª áŠ áŒ¦áˆˆ áŠ áŒ¤áŠ \
    áŒ¥áŒ‹á‰µ áŒ á‹ˆáˆ¨ áŒ áŠá‰ á‹˜ áŒ¥áŠ•áˆµáˆµ áŒ áˆ¨á‰€ áŒ áˆ¨á‰ƒ áŒ“áˆáˆˆ áŒá‹°áˆáˆ¨ áŠáŒá‹° áŒ‰áŠ•á‰áˆ áŒá‰ áŒá‰  áŒáˆ˜á‹µ áŒáˆˆáˆ˜ áŒá‹µáˆá‰µ áŒ‹áˆ áŒ‹áŒ áˆ¨ áŒ‹áŠ•á‹µá‹« áŒ áˆ¨áŠ•áŒˆáˆ áŒ‹áˆ˜áˆ¨ áŒ‹áˆ¨áŒ  á‹°áŠá‰€áˆ¨ áŒ‰áŒáˆµ áŒ‰áŠ•áŒ‰áŠ• áŒ‰á‰£ áŠ®áˆ¨á•á‰³ áŒ‰áˆ«áŒ… áŒáŒáˆ­ á‰°áŒ‹á‹µáˆ \
    áŒˆá‹°á‹° áŒˆá‹³á‹³ áŠ áŠ•áŒ‹á‹°á‹° áŒˆá‹°á‰¥ áŒ‹á‹¨ áŒˆáŠ“á‹˜á‰  áŠ áŒˆáŠ“á‹˜á‰  áŒˆá‰ á‹¨ áˆáŒá‰£áˆ­ áŒˆáˆ¸áˆ¸ á‹°áˆ­á‰£á‰£ áŒˆáˆ«áˆ« áŒˆáˆ˜áŒ áŒ  áŒˆáˆ˜áˆ¨ áŒáˆáŒáˆ áŠ áŒˆáˆˆá‹°áˆ˜ áŒ…áŠ•áŒáˆ‹ á‹¶áˆˆá‹ á‹³á‹³ áŠ á‹°áŒˆá‹°áŒˆ áŠ áŒá‰ á‹°á‹° á‹°á‹¨áŠ á‹°á‹ˆáˆ¨ á‹°áŠ« á‹°áŠ¨áˆ¨ á‹°á‰ áˆ° á‹°áˆ¨á‹˜ á‹°áˆˆáˆáˆ° \
    á‰°áŠ•á‹°áˆ‹á‰€á‰€ á‰°áŠ•á‹ á‰ áˆ¨áˆ¨ á‹áá‰µ á‹˜áŒ áˆ¨ á‹˜áˆá‰€ á‹˜á‹¨áŠ á‹˜áŠá‹˜áŠ á‹˜áˆ­á á‹á‰…áˆ­ á‹‹áŒ€ á‹‹á‰°á‰° á‰£á‰°áˆˆ á‹‹áˆˆáŒˆ á‹‹áˆ­á‹³ á‹ˆáˆáˆ á‰°á‹ˆáŒ£ á‹ˆáŒ‹áŒˆáŠ• á‹ˆá‹°áˆ¨áŠ› á‹ˆáŠªáˆ á‰°á‹ˆáŠ“á‰ á‹° á‹ˆá‰³á‹ áŠ á‹‹á‰€áˆ¨ á‰°á‹ˆá‰€áˆ¨ á‹ˆáˆ¸á‰£ áŠ áˆ˜áˆáˆ›áˆ á‹ˆáˆ¸áŠ” á‹ˆáˆ°á‹ \
    á‹ˆáˆµá‹‹áˆ³ á‹ˆáˆ°áˆ° á‹ˆáˆ‹á‹ˆáˆˆ á‹ˆáˆ‹áˆ‹ á‹ˆáˆáˆ›áˆ› áŠ³áˆ¸ á‰°áŠ®áˆáˆ° áŠ®áˆáŠ• áŠ®á‰³ áŠ©áˆ© áŒ…áŠ•áŠ• á‰áŠ•áŠ• áŠ®áˆ¨áˆ˜á‰° áŠ©áˆ¨áŒƒ áŠ­áˆµá‰°á‰µ áŠ­áˆªáŠ­ áŠ©áŠ­áŠ’ áŒ­áˆ­á‰µ áŠ áŠ¨áˆáˆˆ áŠ¨áˆáˆ¨áˆ¨ á‰€áˆáˆ¨áˆ¨ áŠ áŠ¨áŠáˆáˆ° áŠ áŒá‰ áŒá‰  áŠ¨áŠá‰°áˆ¨ áŠ­á‰¥áˆ­ áˆáŠ¥áˆáŠ“ áŠ¨áˆ°á‰° áŠ¨áˆ¨áŠ¨áˆ¨ \
    áŠ¨áˆˆáˆ áŠ¨áˆˆá‰ áˆ° áŠ¨áˆˆáˆ° áŠ¥á‹µáˆ­ áŠ áˆáŒˆ áŠ áŒ€áˆˆ áŠ á‹µáˆ­á‰£á‹­ áŠ á‹˜á‰…á‰µ áŠ¥áŠ©á‹« áŠ áŠ•áŒƒ áŠ áŠ“á‹°áˆˆ áŠ áŠ“áŒ áˆ¨ áŠ á‰µáˆ‹áˆµ áŠ á‰°á‰° áŠ á‰£á‹œ áŠ á‰¥áŠá‰µ áŠ á‰£áˆª áŠ á‰ á‰€ áŠ áˆµá‰¤á‹› áˆ›áˆ°áˆµ áŠ áˆ¨áŠ•á‹› áŠ áˆ˜áˆ¨á‰ƒ áŠ áˆˆáˆ˜áŒ  áŠáˆáˆˆáˆˆ áŠ áŠ“áŒ á‰  áŠáŒˆáˆ¨áˆáŒ… áŠáŠ®á‰° \
    áˆ˜áˆ¸áŠ¨ áˆ˜áŠ•á‰†áˆ­ áŠ•á‰…áˆµ á‰¶á‰³áŠ• á‰±á‰£ á‰±áŒƒáˆ­ á‰°áŠ á‰…á‰¦ á‰°áŠáŠ áˆ˜áˆáˆ…á‰…"
    root=root.split()
    colum=["word","count"]
    lemma=dataset = pd.DataFrame(columns=colum)
    for w in root:
        lemma.loc[len(lemma.index)]=[w,0]
    lemma.to_excel('dataset/other/complex_word.xlsx',index=False)
    
spch=open("dataset/stoplist/spchar.txt",'r',encoding="utf-8").read().split()
amharicstop=open("dataset/stoplist/amharic_stop_lists.txt",'r',encoding="utf-8").read().split()

def ComplexityAnotator(text):
    sentences=re.split('[?á¢!\n]', text)
    for i in sentences:
        if len(i)<250 and len(i)>10:
            with open('dataset/sentence.txt', 'a',encoding="utf-8") as file:
                file.write(i+"\n")

    #start extructing text
    column=["text","label"]
    lemma = pd.DataFrame() 
    dataset = pd.DataFrame(columns=column)
    complx = pd.DataFrame(columns=column)
    noncomplx = pd.DataFrame(columns=column)
    preproces = pd.DataFrame(columns=column)
    newsentenc=""

    #saved complex annotated data
    if os.path.exists("dataset/dataset.xlsx"):
        complxold = pd.read_excel("dataset/dataset.xlsx")
        os.remove("dataset/dataset.xlsx")
        complx=pd.concat([complxold,complx]) 

    #saved reserved annotated data     
    if os.path.exists("dataset/other/reserve.xlsx"):
        reserves = pd.read_excel("dataset/other/reserve.xlsx")
        os.remove("dataset/other/reserve.xlsx")
        noncomplx=pd.concat([reserves,noncomplx])

    if os.path.exists("dataset/sentence.txt"):
        allfiles = glob.glob('dataset/sentence.txt')#most change simple to sentence
        df = pd.concat((pd.read_csv(f, header=None, names=["text"]) for f in allfiles))
        lemma = pd.read_excel("dataset/other/complex_word.xlsx")
        if df.empty==False:
            for sent in df["text"]:
                catch=""
                rootsent=""
                tokens=sent.split()
                for words in tokens:
                    reslt=""
                    if words not in spch:
                        wordrt=hm.anal('amh', words, um=True)
                        if wordrt!=[]:
                            wordlema=wordrt[0]['lemma'].replace("|", "/")
                            if "/" in wordlema:
                                reslt = re.search('(.*)/', wordlema)
                                reslt=reslt.group(1)
                                rootsent+=reslt+" "
                            else:
                                reslt=wordlema
                                rootsent=rootsent+" "+reslt+" "
                        else:
                            reslt=words
                            rootsent=rootsent+" "+reslt+" "
                    else:
                        reslt=words
                        rootsent=rootsent+" "+reslt+" "
                index=0
                for comp in lemma["word"].values:
                    if " "+comp+" " in rootsent:
                        catch="found"
                        if len(rootsent)<250 and len(rootsent)>10 and lemma.loc[index,'count']<50 and sent not in complx.text.values:
                            lemma.loc[index,'count']=lemma.loc[index,'count']+1
                            print(comp)
                            print(sent)
                            newsentenc="upgraded"
                            complx.loc[len(complx.index)]=[sent,1]#[rootsent,1]
                    index+=1
                if catch=="" and len(sent)<250 and len(sent)>10 and sent not in noncomplx.text.values:
                    noncomplx.loc[len(noncomplx.index)]=[sent,0]
        if newsentenc=="":
            print("No new data found sentencess already exist")

    #delete old complex terms  and save new one
    if os.path.exists("dataset/other/complex_word.xlsx"):
        os.remove("dataset/other/complex_word.xlsx")
        lemma.to_excel('dataset/other/complex_word.xlsx',index=False)

    # Balance dataset size
    reserve = pd.DataFrame(columns=column)
    result=Counter(complx.label.values==1)
    comp=result[True]
    simp=result[False]
    rslt=comp-simp
    c=0
    if rslt>1:
        for i in noncomplx["text"]:
            c+=1
            if c<rslt:
                complx.loc[len(complx.index)]=[i,0]
            else:
                reserve.loc[len(reserve.index)]=[i,0]
    else:
        reserve=pd.concat([reserve,noncomplx]) 
    if os.path.exists("dataset/sentence.txt"):
        os.remove("dataset/sentence.txt")

    complx.to_excel('dataset/dataset.xlsx',index=False)
    reserve.to_excel('dataset/other/reserve.xlsx',index=False)
    
    #Text preprosessing
    if newsentenc!="":
        data=pd.read_excel (r'dataset/dataset.xlsx')
        pros=input("Do You Want to Pre-process the dataset Y/N")
        if pros=="Y" or pros=="y":
            #Remove unexpected char like \ueff
            for indexs, cell_val in enumerate(data["text"].values):
                cell_vals=cell_val.split()
                cell_val=""
                for wrd in cell_vals:
                    if wrd not in spch:
                        cell_val+=wrd+" "
                        data.loc[indexs,'text'] = cell_val

            #remove special characters         
            for indexs, cell_val in enumerate(data["text"].values):
                for i in spch:
                    cell_val=cell_val.replace(i, "")
                data.loc[indexs,'text'] = cell_val

            # remove stopwords
            for index, sentence in enumerate(data["text"].values):
                sentence=sentence.split()
                nonstop_stor=""
                for word in sentence:
                    if word not in amharicstop:
                        nonstop_stor+=word+" "
                if nonstop_stor!="":
                    data.loc[index,'text'] = nonstop_stor
            #Normalize text
            try:
                for index, sentence in enumerate(data["text"].values):
                    normalized = normalizer.normalize(sentence) 
                    data.loc[index,'text'] = normalized
            except Exception as err:
                  print()
            
            #convert to root
            for index, sent in enumerate(data["text"].values):
                rootsent=""
                tokens=sent.split()
                for words in tokens:
                    reslt=""
                    if words not in spch:
                        wordrt=hm.anal('amh', words, um=True)
                        if wordrt!=[]:
                            wordlema=wordrt[0]['lemma'].replace("|", "/")
                            if "/" in wordlema:
                                reslt = re.search('(.*)/', wordlema)
                                reslt=reslt.group(1)
                                rootsent+=reslt+" "
                            else:
                                reslt=wordlema
                                rootsent=rootsent+" "+reslt+" "
                        else:
                            reslt=words
                            rootsent=rootsent+" "+reslt+" "
                    else:
                        reslt=words
                        rootsent=rootsent+" "+reslt+" "
                data.loc[index,'text'] = rootsent   
            if os.path.exists("dataset/preprocessed_data.xlsx"):
                preproces = pd.read_excel("dataset/preprocessed_data.xlsx")
                os.remove("dataset/preprocessed_data.xlsx")
                data=pd.concat([preproces,data])
                data.drop_duplicates()
            data.to_excel('dataset/preprocessed_data.xlsx',index=False)

        #build new vocabulary
        bvcb=input("Do You Want to Build vocab for pretrained Y/N")
        if bvcb=="Y" or bvcb=="y":
            bildvocab()
            vocab=open("dataset/vocab.txt",'r',encoding="utf-8")
            vocab=vocab.read()
            vocab = vocab.split()
            print("building new vocabulary wait...")
            for sent in data["text"]:
                sent=sent.split()
                for word in sent:
                    if word not in vocab:
                        with open('dataset/vocab.txt', 'a',encoding="utf-8") as file:
                            file.write(word+"\n")
                        vocab=open("dataset/vocab.txt",'r',encoding="utf-8")
                        vocab=vocab.read()
                        vocab = vocab.split()
            print("total vocab built: "+str(len(vocab)))
            print("total vdataset size: "+str(len(data)))

    result=Counter(complx.label.values==1)
    comp=result[True]
    simp=result[False]
    total=comp+simp
    complx=0
    simpl=0
    if total>0:
        complx=round((comp/total)*100,1)
        simpl=round((simp/total)*100,1)
    
    print("total dataset:"+str(total))
    print("data distribution: complex "+str(complx)+"%"+" Simple "+str(simpl)+"%")
    if complx>55:
          print("Data imbalancation issue please add more data to balance the distribution")

def bildvocab():
    if not os.path.exists("dataset/vocab.txt"):
        vocab="[PAD] [unused0] [unused1] [unused2] [unused3] [unused4] [unused5] [unused6] [unused7] [unused8] [unused9] [unused10] [unused11] [unused12]\
        [unused13] [unused14] [unused15] [unused16] [unused17] [unused18] [unused19] [unused20] [unused21] [unused22] [unused23] [unused24] [unused25] \
        [unused26] [unused27] [unused28] [unused29] [unused30] [unused31] [unused32] [unused33] [unused34] [unused35] [unused36] [unused37] [unused38] \
        [unused39] [unused40] [unused41] [unused42] [unused43] [unused44] [unused45] [unused46] [unused47] [unused48] [unused49] [unused50] [unused51] \
        [unused52] [unused53] [unused54] [unused55] [unused56] [unused57] [unused58] [unused59] [unused60] [unused61] [unused62] [unused63] [unused64] \
        [unused65] [unused66] [unused67] [unused68] [unused69] [unused70] [unused71] [unused72] [unused73] [unused74] [unused75] [unused76] [unused77] \
        [unused78] [unused79] [unused80] [unused81] [unused82] [unused83] [unused84] [unused85] [unused86] [unused87] [unused88] [unused89] [unused90] \
        [unused91] [unused92] [unused93] [unused94] [unused95] [unused96] [unused97] [unused98] [UNK] [CLS] [SEP] [MASK] [unused99] [unused100] [unused101] \
        [unused102] [unused103] [unused104] [unused105] [unused106] [unused107] [unused108] [unused109] [unused110] [unused111] [unused112] [unused113] \
        [unused114] [unused115] [unused116] [unused117] [unused118] [unused119] [unused120] [unused121] [unused122] [unused123] [unused124] [unused125] \
        [unused126] [unused127] [unused128] [unused129] [unused130] [unused131] [unused132] [unused133] [unused134] [unused135] [unused136] [unused137] \
        [unused138] [unused139] [unused140] [unused141] [unused142] [unused143] [unused144] [unused145] [unused146] [unused147] [unused148] [unused149] \
        [unused150] [unused151] [unused152] [unused153] [unused154] [unused155] [unused156] [unused157] [unused158] [unused159] [unused160] [unused161] \
        [unused162] [unused163] [unused164] [unused165] [unused166] [unused167] [unused168] [unused169] [unused170] [unused171] [unused172] [unused173] \
        [unused174] [unused175] [unused176] [unused177] [unused178] [unused179] [unused180] [unused181] [unused182] [unused183] [unused184] [unused185] \
        [unused186] [unused187] [unused188] [unused189] [unused190] [unused191] [unused192] [unused193] [unused194] [unused195] [unused196] [unused197] \
        [unused198] [unused199] [unused200] [unused201] [unused202] [unused203] [unused204] [unused205] [unused206] [unused207] [unused208] [unused209] \
        [unused210] [unused211] [unused212] [unused213] [unused214] [unused215] [unused216] [unused217] [unused218] [unused219] [unused220] [unused221] \
        [unused222] [unused223] [unused224] [unused225] [unused226] [unused227] [unused228] [unused229] [unused230] [unused231] [unused232] [unused233] \
        [unused234] [unused235] [unused236] [unused237] [unused238] [unused239] [unused240] [unused241] [unused242] [unused243] [unused244] [unused245] \
        [unused246] [unused247] [unused248] [unused249] [unused250] [unused251] [unused252] [unused253] [unused254] [unused255] [unused256] [unused257] \
        [unused258] [unused259] [unused260] [unused261] [unused262] [unused263] [unused264] [unused265] [unused266] [unused267] [unused268] [unused269] \
        [unused270] [unused271] [unused272] [unused273] [unused274] [unused275] [unused276] [unused277] [unused278] [unused279] [unused280] [unused281] \
        [unused282] [unused283] [unused284] [unused285] [unused286] [unused287] [unused288] [unused289] [unused290] [unused291] [unused292] [unused293] \
        [unused294] [unused295] [unused296] [unused297] [unused298] [unused299] [unused300] [unused301] [unused302] [unused303] [unused304] [unused305] \
        [unused306] [unused307] [unused308] [unused309] [unused310] [unused311] [unused312] [unused313] [unused314] [unused315] [unused316] [unused317] \
        [unused318] [unused319] [unused320] [unused321] [unused322] [unused323] [unused324] [unused325] [unused326] [unused327] [unused328] [unused329] \
        [unused330] [unused331] [unused332] [unused333] [unused334] [unused335] [unused336] [unused337] [unused338] [unused339] [unused340] [unused341] \
        [unused342] [unused343] [unused344] [unused345] [unused346] [unused347] [unused348] [unused349] [unused350] [unused351] [unused352] [unused353] \
        [unused354] [unused355] [unused356] [unused357] [unused358] [unused359] [unused360] [unused361] [unused362] [unused363] [unused364] [unused365] \
        [unused366] [unused367] [unused368] [unused369] [unused370] [unused371] [unused372] [unused373] [unused374] [unused375] [unused376] [unused377] \
        [unused378] [unused379] [unused380] [unused381] [unused382] [unused383] [unused384] [unused385] [unused386] [unused387] [unused388] [unused389] \
        [unused390] [unused391] [unused392] [unused393] [unused394] [unused395] [unused396] [unused397] [unused398] [unused399] [unused400] [unused401] \
        [unused402] [unused403] [unused404] [unused405] [unused406] [unused407] [unused408] [unused409] [unused410] [unused411] [unused412] [unused413] \
        [unused414] [unused415] [unused416] [unused417] [unused418] [unused419] [unused420] [unused421] [unused422] [unused423] [unused424] [unused425] \
        [unused426] [unused427] [unused428] [unused429] [unused430] [unused431] [unused432] [unused433] [unused434] [unused435] [unused436] [unused437] \
        [unused438] [unused439] [unused440] [unused441] [unused442] [unused443] [unused444] [unused445] [unused446] [unused447] [unused448] [unused449] \
        [unused450] [unused451] [unused452] [unused453] [unused454] [unused455] [unused456] [unused457] [unused458] [unused459] [unused460] [unused461] \
        [unused462] [unused463] [unused464] [unused465] [unused466] [unused467] [unused468] [unused469] [unused470] [unused471] [unused472] [unused473] \
        [unused474] [unused475] [unused476] [unused477] [unused478] [unused479] [unused480] [unused481] [unused482] [unused483] [unused484] [unused485] \
        [unused486] [unused487] [unused488] [unused489] [unused490] [unused491] [unused492] [unused493] [unused494] [unused495] [unused496] [unused497] \
        [unused498] [unused499] [unused500] [unused501] [unused502] [unused503] [unused504] [unused505] [unused506] [unused507] [unused508] [unused509] \
        [unused510] [unused511] [unused512] [unused513] [unused514] [unused515] [unused516] [unused517] [unused518] [unused519] [unused520] [unused521] \
        [unused522] [unused523] [unused524] [unused525] [unused526] [unused527] [unused528] [unused529] [unused530] [unused531] [unused532] [unused533] \
        [unused534] [unused535] [unused536] [unused537] [unused538] [unused539] [unused540] [unused541] [unused542] [unused543] [unused544] [unused545] \
        [unused546] [unused547] [unused548] [unused549] [unused550] [unused551] [unused552] [unused553] [unused554] [unused555] [unused556] [unused557] \
        [unused558] [unused559] [unused560] [unused561] [unused562] [unused563] [unused564] [unused565] [unused566] [unused567] [unused568] [unused569] \
        [unused570] [unused571] [unused572] [unused573] [unused574] [unused575] [unused576] [unused577] [unused578] [unused579] [unused580] [unused581] \
        [unused582] [unused583] [unused584] [unused585] [unused586] [unused587] [unused588] [unused589] [unused590] [unused591] [unused592] [unused593] \
        [unused594] [unused595] [unused596] [unused597] [unused598] [unused599] [unused600] [unused601] [unused602] [unused603] [unused604] [unused605] \
        [unused606] [unused607] [unused608] [unused609] [unused610] [unused611] [unused612] [unused613] [unused614] [unused615] [unused616] [unused617] \
        [unused618] [unused619] [unused620] [unused621] [unused622] [unused623] [unused624] [unused625] [unused626] [unused627] [unused628] [unused629] \
        [unused630] [unused631] [unused632] [unused633] [unused634] [unused635] [unused636] [unused637] [unused638] [unused639] [unused640] [unused641] \
        [unused642] [unused643] [unused644] [unused645] [unused646] [unused647] [unused648] [unused649] [unused650] [unused651] [unused652] [unused653] \
        [unused654] [unused655] [unused656] [unused657] [unused658] [unused659] [unused660] [unused661] [unused662] [unused663] [unused664] [unused665] \
        [unused666] [unused667] [unused668] [unused669] [unused670] [unused671] [unused672] [unused673] [unused674] [unused675] [unused676] [unused677] \
        [unused678] [unused679] [unused680] [unused681] [unused682] [unused683] [unused684] [unused685] [unused686] [unused687] [unused688] [unused689] \
        [unused690] [unused691] [unused692] [unused693] [unused694] [unused695] [unused696] [unused697] [unused698] [unused699] [unused700] [unused701] \
        [unused702] [unused703] [unused704] [unused705] [unused706] [unused707] [unused708] [unused709] [unused710] [unused711] [unused712] [unused713] \
        [unused714] [unused715] [unused716] [unused717] [unused718] [unused719] [unused720] [unused721] [unused722] [unused723] [unused724] [unused725] \
        [unused726] [unused727] [unused728] [unused729] [unused730] [unused731] [unused732] [unused733] [unused734] [unused735] [unused736] [unused737] \
        [unused738] [unused739] [unused740] [unused741] [unused742] [unused743] [unused744] [unused745] [unused746] [unused747] [unused748] [unused749] \
        [unused750] [unused751] [unused752] [unused753] [unused754] [unused755] [unused756] [unused757] [unused758] [unused759] [unused760] [unused761] \
        [unused762] [unused763] [unused764] [unused765] [unused766] [unused767] [unused768] [unused769] [unused770] [unused771] [unused772] [unused773] \
        [unused774] [unused775] [unused776] [unused777] [unused778] [unused779] [unused780] [unused781] [unused782] [unused783] [unused784] [unused785] \
        [unused786] [unused787] [unused788] [unused789] [unused790] [unused791] [unused792] [unused793] [unused794] [unused795] [unused796] [unused797] \
        [unused798] [unused799] [unused800] [unused801] [unused802] [unused803] [unused804] [unused805] [unused806] [unused807] [unused808] [unused809] \
        [unused810] [unused811] [unused812] [unused813] [unused814] [unused815] [unused816] [unused817] [unused818] [unused819] [unused820] [unused821] \
        [unused822] [unused823] [unused824] [unused825] [unused826] [unused827] [unused828] [unused829] [unused830] [unused831] [unused832] [unused833] \
        [unused834] [unused835] [unused836] [unused837] [unused838] [unused839] [unused840] [unused841] [unused842] [unused843] [unused844] [unused845] \
        [unused846] [unused847] [unused848] [unused849] [unused850] [unused851] [unused852] [unused853] [unused854] [unused855] [unused856] [unused857] \
        [unused858] [unused859] [unused860] [unused861] [unused862] [unused863] [unused864] [unused865] [unused866] [unused867] [unused868] [unused869] \
        [unused870] [unused871] [unused872] [unused873] [unused874] [unused875] [unused876] [unused877] [unused878] [unused879] [unused880] [unused881] \
        [unused882] [unused883] [unused884] [unused885] [unused886] [unused887] [unused888] [unused889] [unused890] [unused891] [unused892] [unused893] \
        [unused894] [unused895] [unused896] [unused897] [unused898] [unused899] [unused900] [unused901] [unused902] [unused903] [unused904] [unused905] \
        [unused906] [unused907] [unused908] [unused909] [unused910] [unused911] [unused912] [unused913] [unused914] [unused915] [unused916] [unused917] \
        [unused918] [unused919] [unused920] [unused921] [unused922] [unused923] [unused924] [unused925] [unused926] [unused927] [unused928] [unused929] \
        [unused930] [unused931] [unused932] [unused933] [unused934] [unused935] [unused936] [unused937] [unused938] [unused939] [unused940] [unused941] \
        [unused942] [unused943] [unused944] [unused945] [unused946] [unused947] [unused948] [unused949] [unused950] [unused951] [unused952] [unused953] \
        [unused954] [unused955] [unused956] [unused957] [unused958] [unused959] [unused960] [unused961] [unused962] [unused963] [unused964] [unused965] \
        [unused966] [unused967] [unused968] [unused969] [unused970] [unused971] [unused972] [unused973] [unused974] [unused975] [unused976] [unused977] \
        [unused978] [unused979] [unused980] [unused981] [unused982] [unused983] [unused984] [unused985] [unused986] [unused987] [unused988] [unused989] \
        [unused990] [unused991] [unused992] [unused993]"
        vocab = vocab.split()
        with open('dataset/vocab.txt', 'a',encoding="utf-8") as file:
            for i in vocab:
                file.write(i+"\n")
                
def amTextPreprocessing(textss):
    colum=["text"]
    data = pd.DataFrame(columns=colum)
    sentences=re.split('[?á¢!\n]', textss)
    for i in sentences:
        data.loc[len(data.index)]=[i]

    #Remove unexpected char like \ueff
    for indexs, cell_val in enumerate(data["text"].values):
        cell_vals=cell_val.split()
        cell_val=""
        for wrd in cell_vals:
            if wrd not in spch:
                cell_val+=wrd+" "
                data.loc[indexs,'text'] = cell_val

    #remove special characters         
    for indexs, cell_val in enumerate(data["text"].values):
        for i in spch:
            cell_val=cell_val.replace(i, "")
        data.loc[indexs,'text'] = cell_val

    # remove stopwords
    for index, sentence in enumerate(data["text"].values):
        sentence=sentence.split()
        nonstop_stor=""
        for word in sentence:
            if word not in amharicstop:
                nonstop_stor+=word+" "
        if nonstop_stor!="":
            data.loc[index,'text'] = nonstop_stor
    #Normalize text
    try:
        for index, sentence in enumerate(data["text"].values):
            normalized = normalizer.normalize(sentence) 
            data.loc[index,'text'] = normalized
    except Exception as err:
          print()
    #convert to root
    for index, sent in enumerate(data["text"].values):
        rootsent=""
        tokens=sent.split()
        for words in tokens:
            reslt=""
            if words not in spch:
                wordrt=hm.anal('amh', words, um=True)
                if wordrt!=[]:
                    wordlema=wordrt[0]['lemma'].replace("|", "/")
                    if "/" in wordlema:
                        reslt = re.search('(.*)/', wordlema)
                        reslt=reslt.group(1)
                        rootsent+=reslt+" "
                    else:
                        reslt=wordlema
                        rootsent=rootsent+" "+reslt+" "
                else:
                    reslt=words
                    rootsent=rootsent+" "+reslt+" "
            else:
                reslt=words
                rootsent=rootsent+" "+reslt+" "
        data.loc[index,'text'] = rootsent   
    for sent in data["text"]:
          if sent !="":
            with open('dataset/preprocesseddata.txt', 'a',encoding="utf-8") as file:
                file.write(sent+"\n")


